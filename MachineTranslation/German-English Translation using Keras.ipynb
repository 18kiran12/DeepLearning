{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump \n",
    "import re\n",
    "import numpy as np\n",
    "from unicodedata import normalize\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Read data</h3>\n",
    "<p>Data from http://www.manythings.org/, German words and their translations</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = load_doc('data/deu-eng/deu.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi.\\tHallo!\\nHi.\\tGrüß '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cleaning the data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pairs(doc):\n",
    "    lines = doc.split('\\n')\n",
    "    pairs = [x.split('\\t') for x in lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hi.', 'Hallo!'],\n",
       " ['Hi.', 'Grüß Gott!'],\n",
       " ['Run!', 'Lauf!'],\n",
       " ['Fire!', 'Feuer!'],\n",
       " ['Help!', 'Hilfe!']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = to_pairs(doc)\n",
    "pairs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Remove all non-printable characters.\n",
    "- Remove all punctuation characters.\n",
    "- Normalize all Unicode characters to ASCII (e.g. Latin characters).\n",
    "- Normalize the case to lowercase.\n",
    "- Remove any remaining tokens that are not alphabetic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return np.array(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unicodedata module offers a .normalize() function, you want to normalize to the NFC form. NFC, or 'Normal Form Composed' returns composed characters, NFD, 'Normal Form Decomposed' gives you decomposed, combined characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['hi', 'hallo']), list(['hi', 'gru gott']),\n",
       "       list(['run', 'lauf']), list(['fire', 'feuer']),\n",
       "       list(['help', 'hilfe'])], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_pairs = clean_pairs(pairs)\n",
    "cleaned_pairs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Save the cleansed data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/deu-eng/cleansed_deu.pkl\n"
     ]
    }
   ],
   "source": [
    "save_clean_data(cleaned_pairs, \"data/deu-eng/cleansed_deu.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading the processed data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_clean_sentences('data/deu-eng/cleansed_deu.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['hi', 'hallo']), list(['hi', 'gru gott']),\n",
       "       list(['run', 'lauf']), list(['fire', 'feuer']),\n",
       "       list(['help', 'hilfe'])], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169814"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenize</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = pd.DataFrame([x for x in raw_dataset[:N]], columns = ['English', 'German'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_length = max_length(raw_dataset[\"English\"].tolist())\n",
    "de_length = max_length(raw_dataset[\"German\"].tolist())\n",
    "en_length, de_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3753, 5814)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokenizer = create_tokenizer(raw_dataset[\"English\"].tolist())\n",
    "de_tokenizer = create_tokenizer(raw_dataset[\"German\"].tolist())\n",
    "en_vocab_size = len(en_tokenizer.word_index) + 1\n",
    "de_vocab_size = len(de_tokenizer.word_index) + 1\n",
    "en_vocab_size, de_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Viewing output of tokenizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tom</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "tom  1\n",
       "i    2\n",
       "is   3\n",
       "you  4\n",
       "it   5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_token_mapping = pd.DataFrame.from_dict(en_tokenizer.word_index, orient = 'index')\n",
    "en_token_mapping[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the keras preprocessing tokenizes the english text, now we do the same for german texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test Train split</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floor(n):\n",
    "    res = int(n)\n",
    "    return res if res == n or n >= 0 else res-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marks: [6000, 12000, 18000]\n"
     ]
    }
   ],
   "source": [
    "def index_marks(nrows, chunk_size):\n",
    "    return range(1 * chunk_size, (nrows // chunk_size + 1) * chunk_size, chunk_size)\n",
    "\n",
    "indices = list(index_marks(len(raw_dataset), 6000))\n",
    "print(\"Marks: {}\".format(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(data, test_frac, indices):\n",
    "    n = len(data)\n",
    "    #shuffle the dataset\n",
    "    data = shuffle(data)\n",
    "    #calc row no from test_frac\n",
    "    test_no = floor(n * test_frac)\n",
    "    test = data[:test_no]\n",
    "    print(\"len of testing data :\", len(test))\n",
    "    \n",
    "    #for training data\n",
    "    data = shuffle(data[test_no:])\n",
    "    print(\"len of training data :\", len(data))\n",
    "    train = np.array_split(data, indices)\n",
    "    return test, train, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of testing data : 2000\n",
      "len of training data : 18000\n"
     ]
    }
   ],
   "source": [
    "test, train, complete_train = test_train_split(raw_dataset, 0.1, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                 English                     German\n",
       " 6705      the birds sang           die vogel sangen\n",
       " 9732      these are toms            die gehoren tom\n",
       " 11467   do you play golf            spielt ihr golf\n",
       " 10992     you arent here        ihr seid nicht hier\n",
       " 15234  are you the mayor  bist du der burgermeister, 6000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][:5], len(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                English                      German\n",
       " 14471  tom was unafraid       tom hatte keine angst\n",
       " 582            im dying      ich werde bald sterben\n",
       " 11869  here is your bag        hier ist ihre tasche\n",
       " 3827      i am to blame         es ist meine schuld\n",
       " 4358      just check it  kontrollieren sies einfach, 2000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:5], len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Save the train test data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_pickle(\"test.h5\")\n",
    "i = 1\n",
    "for chunks in train:\n",
    "    chunks.to_pickle(\"train_\" + str(i) +\".h5\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_train.to_pickle(\"complete_train.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_train.to_csv( \"complete_train.txt\", sep='|', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating a class for data generation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID be the Python string that identifies a given sample of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load train test data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"test.h5\")\n",
    "complete_train = pd.read_pickle(\"complete_train.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tokenized the entire dataset, now we need to apply this to the training and testing dataset using the tokenizer created. We need to encode each sequence to max length to be used as a word embedding. Keras pad sequence helps to pad all sequences into similar length. 'post' padding option allows us to add 0's after each sequence to complete the max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seq(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_matrix(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final layer would be a probablity distribution of each word appearing in the sentence and hence has to be one hot encoded layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "#Converts a class vector (integers) to binary class matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_output(sequence, vocab_size):\n",
    "    ylist = list()\n",
    "    encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "    ylist.append(encoded)\n",
    "    y = np.array(ylist)\n",
    "    y = y.reshape(sequence.shape[0], sequence.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_arrays_from_file(path):\n",
    "#     while True:\n",
    "#             with open(path) as f:\n",
    "#                 for line in f:\n",
    "#                     x, y = line.split(\"|\")\n",
    "#                     y = y.strip(\"\\n\")\n",
    "#                     print(x,\" | \",  y)\n",
    "#                     break\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a data generator to call data in batches\n",
    "\n",
    "def generate_arrays_from_file(path, en_tokenizer, en_length, de_vocab_size):\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            # create numpy arrays of input data\n",
    "            # and labels, from each line in the file\n",
    "            trainX, trainY = line.split(\"|\")\n",
    "            trainY = trainY.strip(\"\\n\")\n",
    "\n",
    "            print(trainX)\n",
    "\n",
    "            trainX = encode_seq(en_tokenizer, en_length, trainX)\n",
    "            trainY = encode_seq(de_tokenizer, de_length, trainY)\n",
    "            print(trainX.shape)\n",
    "\n",
    "            trainY = encode_output(trainY, de_vocab_size)\n",
    "\n",
    "            return ({'input_1': trainX}, {'output': trainY})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im useless\n",
      "(10, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'input_1': array([[0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0]], dtype=int32)},\n",
       " {'output': array([[[1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.]],\n",
       "  \n",
       "         [[1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.]],\n",
       "  \n",
       "         [[1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.]],\n",
       "  \n",
       "         [[1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.]],\n",
       "  \n",
       "         [[1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.],\n",
       "          [1., 0., 0., ..., 0., 0., 0.]]])})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_arrays_from_file(\"complete_train.txt\", en_tokenizer, en_length, de_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainY = encode_output(trainY[:5000], de_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10, 3686)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainY[:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainY[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = encode_seq(en_tokenizer, en_length, test[\"English\"].tolist())\n",
    "testY = encode_seq(de_tokenizer, de_length, test[\"German\"].tolist())\n",
    "testY = encode_output(testY, de_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units)) #encoder\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = def_model(en_vocab_size, de_vocab_size, en_length, de_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy') #use adam to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 6, 256)            960768    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 10, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 10, 256)           525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 10, 5814)          1494198   \n",
      "=================================================================\n",
      "Total params: 3,505,590\n",
      "Trainable params: 3,505,590\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must connect the encoder to the decoder, and they do not fit.\n",
    "\n",
    "That is, the encoder will produce a 2-dimensional matrix of outputs, where the length is defined by the number of memory cells in the layer. The decoder is an LSTM layer that expects a 3D input of [samples, time steps, features] in order to produce a decoded sequence of some different length defined by the problem.\n",
    "\n",
    "If you try to force these pieces together, you get an error indicating that the output of the decoder is 2D and 3D input to the decoder is required.\n",
    "\n",
    "We can solve this using a <b>RepeatVector layer</b>. This layer simply repeats the provided 2D input multiple times to create a 3D output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Save the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No data provided for \"embedding_1_input\". Need data for each key in: ['embedding_1_input']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'embedding_1_input'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-1aeeb9c85aac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_arrays_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'complete_train.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mde_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1251\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2242\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2243\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2244\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1882\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1884\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1885\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1886\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1481\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1483\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1484\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1485\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     65\u001b[0m             raise ValueError(\n\u001b[1;32m     66\u001b[0m                 \u001b[0;34m'No data provided for \"'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\". Need data '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 'for each key in: ' + str(names))\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No data provided for \"embedding_1_input\". Need data for each key in: ['embedding_1_input']"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generate_arrays_from_file('complete_train.txt', de_vocab_size), steps_per_epoch=6000, epochs=10, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading the saved model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = model.predict(testX, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.2011976e-06, 8.1534272e-01, 1.6022379e-02, ..., 1.5784324e-09,\n",
       "        2.6694651e-09, 2.1605746e-09],\n",
       "       [2.4934425e-05, 5.0718516e-02, 9.4015925e-04, ..., 3.8430712e-09,\n",
       "        6.1699046e-09, 4.5124149e-09],\n",
       "       [3.8303973e-04, 3.1723335e-02, 8.1044942e-02, ..., 1.0078786e-08,\n",
       "        1.7822506e-08, 1.1264025e-08],\n",
       "       ...,\n",
       "       [9.9778193e-01, 4.0095761e-06, 1.0868539e-05, ..., 9.6096056e-12,\n",
       "        2.1693572e-11, 1.1184460e-11],\n",
       "       [9.9870932e-01, 1.6168731e-06, 4.6534847e-06, ..., 5.0155565e-12,\n",
       "        1.1453427e-11, 6.0343579e-12],\n",
       "       [9.9913824e-01, 8.7311832e-07, 2.4852766e-06, ..., 3.3935276e-12,\n",
       "        7.4892124e-12, 4.0524363e-12]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation[0] #probablity distribution of first sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform this mapping for each integer in the translation and return the result as a string of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [np.argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will select the word with maximum probablity and then append to get the predicted sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_sequence(model, de_tokenizer, testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation for machine translation is mainle done using a score called BLEU https://www.aclweb.org/anthology/P02-1040.pdf. For this we need to import corpus_bleu from nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, tokenizer, source)\n",
    "        raw_target = [x[1] for x in dataset][i]\n",
    "        raw_src = [x[0] for x in dataset][i]\n",
    "        if i < 5:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append(raw_target.split())\n",
    "        predicted.append(translation.split())\n",
    "    # calculate BLEU score\n",
    "#     print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "#     print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "#     print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "#     print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src=[tom got mugged], target=[tom wurde uberfallen], predicted=[ich habe es gut]\n",
      "src=[you can rest], target=[sie konnen sich ausruhen], predicted=[du sind gut]\n",
      "src=[did you do that], target=[habt ihr das getan], predicted=[tom ist]\n",
      "src=[this is new], target=[das ist neu], predicted=[tom hat mich]\n",
      "src=[tom is nervous], target=[tom ist nervos], predicted=[ich mag sehr]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, de_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 1001   18  166    0]\n",
      " [  26  356    0    0    0]\n",
      " [   1  456  593    0    0]\n",
      " [   1 2248    8    0    0]\n",
      " [   2   33  494    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(testX[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list(['i led the way', 'ich ging voran'])\n",
      " list(['youre worried', 'ihr macht euch sorgen'])\n",
      " list(['tom drank milk', 'tom trank milch'])\n",
      " list(['tom punched me', 'tom hat mich geschlagen'])\n",
      " list(['i like wine', 'ich mag wein'])]\n"
     ]
    }
   ],
   "source": [
    "print(test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
