{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump \n",
    "import re\n",
    "import numpy as np\n",
    "from unicodedata import normalize\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Read data</h3>\n",
    "<p>Data from http://www.manythings.org/, German words and their translations</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = load_doc('data/deu-eng/deu.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi.\\tHallo!\\nHi.\\tGrüß '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cleaning the data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pairs(doc):\n",
    "    lines = doc.split('\\n')\n",
    "    pairs = [x.split('\\t') for x in lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hi.', 'Hallo!'],\n",
       " ['Hi.', 'Grüß Gott!'],\n",
       " ['Run!', 'Lauf!'],\n",
       " ['Fire!', 'Feuer!'],\n",
       " ['Help!', 'Hilfe!']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = to_pairs(doc)\n",
    "pairs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Remove all non-printable characters.\n",
    "- Remove all punctuation characters.\n",
    "- Normalize all Unicode characters to ASCII (e.g. Latin characters).\n",
    "- Normalize the case to lowercase.\n",
    "- Remove any remaining tokens that are not alphabetic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return np.array(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unicodedata module offers a .normalize() function, you want to normalize to the NFC form. NFC, or 'Normal Form Composed' returns composed characters, NFD, 'Normal Form Decomposed' gives you decomposed, combined characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['hi', 'hallo']), list(['hi', 'gru gott']),\n",
       "       list(['run', 'lauf']), list(['fire', 'feuer']),\n",
       "       list(['help', 'hilfe'])], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_pairs = clean_pairs(pairs)\n",
    "cleaned_pairs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Save the cleansed data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/deu-eng/cleansed_deu.pkl\n"
     ]
    }
   ],
   "source": [
    "save_clean_data(cleaned_pairs, \"data/deu-eng/cleansed_deu.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading the processed data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_clean_sentences('data/deu-eng/cleansed_deu.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['hi', 'hallo']), list(['hi', 'gru gott']),\n",
       "       list(['run', 'lauf']), list(['fire', 'feuer']),\n",
       "       list(['help', 'hilfe'])], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169814"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test Train Split</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sentences = 10000\n",
    "dataset = raw_dataset[:n_sentences]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:9000], dataset[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([list(['i like to dream', 'ich traume gern']),\n",
       "        list(['i look like tom', 'ich sehe wie tom aus']),\n",
       "        list(['boys are stupid', 'jungs sind doof']),\n",
       "        list(['its been tried', 'es ist versucht worden']),\n",
       "        list(['tom is safe', 'tom ist in sicherheit'])], dtype=object), 9000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:5], len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([list(['im not home', 'ich bin nicht zuhause']),\n",
       "        list(['heres the bill', 'hier ist die rechnung']),\n",
       "        list(['how old are you', 'wie alt bist du']),\n",
       "        list(['id like a fork', 'ich hatte gerne eine gabel']),\n",
       "        list(['toms finished', 'tom ist fix und fertig'])], dtype=object),\n",
       " 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:5], len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenize</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just get the english datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i like to dream', 'i look like tom', 'boys are stupid', 'its been tried']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_lines = [x[0] for x in dataset]\n",
    "english_lines[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_length = max_length(english_lines)\n",
    "eng_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = create_tokenizer(english_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(eng_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2315"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_token_mapping = DataFrame.from_dict(eng_tokenizer.word_index, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tom</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "tom  1\n",
       "i    2\n",
       "it   3\n",
       "is   4\n",
       "you  5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_token_mapping[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the keras preprocessing tokenizes the english text, now we do the same for german texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ich</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tom</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ist</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sie</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "ich  1\n",
       "tom  2\n",
       "ist  3\n",
       "sie  4\n",
       "es   5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_lines = [x[1] for x in dataset]\n",
    "de_tokenizer = create_tokenizer(de_lines)\n",
    "de_token_mapping = DataFrame.from_dict(de_tokenizer.word_index, orient = 'index')\n",
    "de_token_mapping[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_length = max_length(de_lines)\n",
    "de_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3686"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_vocab_size = len(de_tokenizer.word_index) + 1\n",
    "de_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tokenized the entire dataset, now we need to apply this to the training and testing dataset using the tokenizer created. We need to encode each sequence to max length to be used as a word embedding. Keras pad sequence helps to pad all sequences into similar length. 'post' padding option allows us to add 0's after each sequence to complete the max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seq(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = encode_seq(de_tokenizer, de_length, [x[1] for x in train])\n",
    "trainX = encode_seq(eng_tokenizer, eng_length, [x[0] for x in train])\n",
    "\n",
    "testY = encode_seq(de_tokenizer, de_length, [x[1] for x in test])\n",
    "testX = encode_seq(eng_tokenizer, eng_length, [x[0] for x in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2,  33,  15, 386,   0],\n",
       "       [  2,  63,  33,   1,   0],\n",
       "       [848,  17, 218,   0,   0],\n",
       "       [ 10, 476, 321,   0,   0],\n",
       "       [  1,   4, 246,   0,   0]], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final layer would be a probablity distribution of each word appearing in the sentence and hence has to be one hot encoded layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX[:1000]\n",
    "testX = testX[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "#Converts a class vector (integers) to binary class matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = np.array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = encode_output(trainY[:1000], de_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10, 3686)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY[:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY = encode_output(testY[:100], de_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units)) #encoder\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = def_model(eng_vocab_size, de_vocab_size, eng_length, de_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy') #use adam to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 5, 256)            592640    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 10, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 10, 256)           525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 10, 3686)          947302    \n",
      "=================================================================\n",
      "Total params: 2,590,566\n",
      "Trainable params: 2,590,566\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must connect the encoder to the decoder, and they do not fit.\n",
    "\n",
    "That is, the encoder will produce a 2-dimensional matrix of outputs, where the length is defined by the number of memory cells in the layer. The decoder is an LSTM layer that expects a 3D input of [samples, time steps, features] in order to produce a decoded sequence of some different length defined by the problem.\n",
    "\n",
    "If you try to force these pieces together, you get an error indicating that the output of the decoder is 2D and 3D input to the decoder is required.\n",
    "\n",
    "We can solve this using a <b>RepeatVector layer</b>. This layer simply repeats the provided 2D input multiple times to create a 3D output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Save the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/30\n",
      " - 21s - loss: 7.4668 - val_loss: 3.9479\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.94786, saving model to model.h5\n",
      "Epoch 2/30\n",
      " - 2s - loss: 2.9538 - val_loss: 2.9843\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.94786 to 2.98433, saving model to model.h5\n",
      "Epoch 3/30\n",
      " - 2s - loss: 2.6483 - val_loss: 2.8775\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.98433 to 2.87746, saving model to model.h5\n",
      "Epoch 4/30\n",
      " - 2s - loss: 2.4780 - val_loss: 2.7522\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.87746 to 2.75224, saving model to model.h5\n",
      "Epoch 5/30\n",
      " - 2s - loss: 2.3264 - val_loss: 2.6526\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.75224 to 2.65262, saving model to model.h5\n",
      "Epoch 6/30\n",
      " - 2s - loss: 2.2359 - val_loss: 2.5603\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.65262 to 2.56027, saving model to model.h5\n",
      "Epoch 7/30\n",
      " - 2s - loss: 2.1370 - val_loss: 2.5140\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.56027 to 2.51402, saving model to model.h5\n",
      "Epoch 8/30\n",
      " - 2s - loss: 2.0750 - val_loss: 2.4859\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.51402 to 2.48593, saving model to model.h5\n",
      "Epoch 9/30\n",
      " - 2s - loss: 2.0396 - val_loss: 2.4618\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.48593 to 2.46179, saving model to model.h5\n",
      "Epoch 10/30\n",
      " - 2s - loss: 1.9981 - val_loss: 2.4425\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.46179 to 2.44251, saving model to model.h5\n",
      "Epoch 11/30\n",
      " - 2s - loss: 1.9771 - val_loss: 2.4316\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.44251 to 2.43165, saving model to model.h5\n",
      "Epoch 12/30\n",
      " - 2s - loss: 1.9504 - val_loss: 2.4341\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/30\n",
      " - 2s - loss: 1.9305 - val_loss: 2.4242\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.43165 to 2.42418, saving model to model.h5\n",
      "Epoch 14/30\n",
      " - 2s - loss: 1.9052 - val_loss: 2.4159\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.42418 to 2.41593, saving model to model.h5\n",
      "Epoch 15/30\n",
      " - 2s - loss: 1.8883 - val_loss: 2.4183\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/30\n",
      " - 2s - loss: 1.8732 - val_loss: 2.4084\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.41593 to 2.40838, saving model to model.h5\n",
      "Epoch 17/30\n",
      " - 2s - loss: 1.8660 - val_loss: 2.4174\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/30\n",
      " - 2s - loss: 1.8449 - val_loss: 2.4056\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.40838 to 2.40560, saving model to model.h5\n",
      "Epoch 19/30\n",
      " - 2s - loss: 1.8337 - val_loss: 2.4066\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/30\n",
      " - 2s - loss: 1.8203 - val_loss: 2.4033\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.40560 to 2.40331, saving model to model.h5\n",
      "Epoch 21/30\n",
      " - 2s - loss: 1.8068 - val_loss: 2.4000\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.40331 to 2.39995, saving model to model.h5\n",
      "Epoch 22/30\n",
      " - 2s - loss: 1.7981 - val_loss: 2.4006\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/30\n",
      " - 2s - loss: 1.7847 - val_loss: 2.3993\n",
      "\n",
      "Epoch 00023: val_loss improved from 2.39995 to 2.39933, saving model to model.h5\n",
      "Epoch 24/30\n",
      " - 2s - loss: 1.7732 - val_loss: 2.4020\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/30\n",
      " - 2s - loss: 1.7632 - val_loss: 2.3988\n",
      "\n",
      "Epoch 00025: val_loss improved from 2.39933 to 2.39879, saving model to model.h5\n",
      "Epoch 26/30\n",
      " - 2s - loss: 1.7522 - val_loss: 2.3977\n",
      "\n",
      "Epoch 00026: val_loss improved from 2.39879 to 2.39772, saving model to model.h5\n",
      "Epoch 27/30\n",
      " - 2s - loss: 1.7404 - val_loss: 2.3956\n",
      "\n",
      "Epoch 00027: val_loss improved from 2.39772 to 2.39560, saving model to model.h5\n",
      "Epoch 28/30\n",
      " - 2s - loss: 1.7276 - val_loss: 2.4017\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/30\n",
      " - 2s - loss: 1.7171 - val_loss: 2.3980\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/30\n",
      " - 2s - loss: 1.7043 - val_loss: 2.3952\n",
      "\n",
      "Epoch 00030: val_loss improved from 2.39560 to 2.39521, saving model to model.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe5250cfcc0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading the saved model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = model.predict(testX, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.42604762e-04, 3.43670785e-01, 1.20422654e-01, ...,\n",
       "        5.01693989e-07, 2.86902093e-07, 4.53218547e-07],\n",
       "       [7.61445379e-04, 3.56775448e-02, 3.07792313e-02, ...,\n",
       "        2.15923649e-07, 8.81704665e-08, 1.52009363e-07],\n",
       "       [5.00156265e-03, 4.55194898e-03, 1.85940862e-02, ...,\n",
       "        2.28333334e-07, 8.08500076e-08, 1.53478595e-07],\n",
       "       ...,\n",
       "       [9.91239607e-01, 7.55340295e-07, 2.28405406e-04, ...,\n",
       "        3.51157159e-10, 1.03764046e-10, 1.41254689e-10],\n",
       "       [9.91378188e-01, 7.37682228e-07, 2.24740521e-04, ...,\n",
       "        3.43785223e-10, 1.01556055e-10, 1.38058412e-10],\n",
       "       [9.91397500e-01, 7.35363869e-07, 2.24252915e-04, ...,\n",
       "        3.42727347e-10, 1.01241626e-10, 1.37596587e-10]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation[0] #probablity distribution of first sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform this mapping for each integer in the translation and return the result as a string of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [np.argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will select the word with maximum probablity and then append to get the predicted sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_sequence(model, de_tokenizer, testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ich ist nicht'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation for machine translation is mainle done using a score called BLEU https://www.aclweb.org/anthology/P02-1040.pdf. For this we need to import corpus_bleu from nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, tokenizer, source)\n",
    "        raw_target = [x[1] for x in dataset][i]\n",
    "        raw_src = [x[0] for x in dataset][i]\n",
    "        if i < 5:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append(raw_target.split())\n",
    "        predicted.append(translation.split())\n",
    "    # calculate BLEU score\n",
    "#     print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "#     print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "#     print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "#     print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src=[i like to dream], target=[ich traume gern], predicted=[ich ist nicht nicht]\n",
      "src=[i look like tom], target=[ich sehe wie tom aus], predicted=[ich ist nicht]\n",
      "src=[boys are stupid], target=[jungs sind doof], predicted=[ich ist nicht]\n",
      "src=[its been tried], target=[es ist versucht worden], predicted=[tom ist]\n",
      "src=[tom is safe], target=[tom ist in sicherheit], predicted=[tom ist]\n",
      "src=[i hurried home], target=[ich eilte nach hause], predicted=[tom ist nicht]\n",
      "src=[im miserable], target=[ich bin unglucklich], predicted=[ich ist nicht]\n",
      "src=[you are crazy], target=[du bist verruckt], predicted=[tom sie]\n",
      "src=[tom looks well], target=[tom sieht gut aus], predicted=[ich ist nicht]\n",
      "src=[keep it up], target=[nicht nachlassen], predicted=[ich ist nicht]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, de_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   7   28   49    0    0]\n",
      " [ 368   18 1064    0    0]\n",
      " [  51  197   17    5    0]\n",
      " [ 265   33    6 2235    0]\n",
      " [  38  802    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(testX[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list(['im not home', 'ich bin nicht zuhause'])\n",
      " list(['heres the bill', 'hier ist die rechnung'])\n",
      " list(['how old are you', 'wie alt bist du'])\n",
      " list(['id like a fork', 'ich hatte gerne eine gabel'])\n",
      " list(['toms finished', 'tom ist fix und fertig'])]\n"
     ]
    }
   ],
   "source": [
    "print(test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
